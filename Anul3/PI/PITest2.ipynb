{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1**"
      ],
      "metadata": {
        "id": "zcXnnM9ecr-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "torch.manual_seed(42);"
      ],
      "metadata": {
        "id": "yKdfAE6ckjfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download(url, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file, return the local filename.\"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "        return fname\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def download_extract(url, folder=None):\n",
        "    \"\"\"Download and extract a zip file.\"\"\"\n",
        "    fname = download(url)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def read_data_bananas(is_train=True):\n",
        "    \"\"\"Read the banana detection dataset images and labels.\"\"\"\n",
        "    data_dir = download_extract('http://d2l-data.s3-accelerate.amazonaws.com/banana-detection.zip')\n",
        "    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train\n",
        "                             else 'bananas_val', 'label.csv')\n",
        "    csv_data = pd.read_csv(csv_fname)\n",
        "    csv_data = csv_data.set_index('img_name')\n",
        "    images, targets = [], []\n",
        "    for img_name, target in csv_data.iterrows():\n",
        "        images.append(torchvision.io.read_image(\n",
        "            os.path.join(data_dir, 'bananas_train' if is_train else\n",
        "                         'bananas_val', 'images', f'{img_name}')))\n",
        "\n",
        "        targets.append(list(target))\n",
        "    return images, torch.tensor(targets).unsqueeze(1) / 256\n",
        "\n",
        "class BananasDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A customized dataset to load the banana detection dataset.\"\"\"\n",
        "    def __init__(self, is_train):\n",
        "        self.features, self.labels = read_data_bananas(is_train)\n",
        "        print('read ' + str(len(self.features)) + (f' training examples' if\n",
        "              is_train else f' validation examples'))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.features[idx].float(), self.labels[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "def load_data_bananas(batch_size):\n",
        "    \"\"\"Load the banana detection dataset.\"\"\"\n",
        "    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),\n",
        "                                             batch_size, shuffle=True)\n",
        "    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),\n",
        "                                           batch_size)\n",
        "    return train_iter, val_iter\n",
        "\n"
      ],
      "metadata": {
        "id": "0PhqnoUilXV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZif-qHqaqo9",
        "outputId": "987a27c9-850b-4ffe-fcbd-966ec08d3bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read 1000 training examples\n",
            "read 100 validation examples\n",
            "Epoch 1, Class accuracy 96.14, Bounding box MAE 5.74e-03\n",
            "Epoch 2, Class accuracy 99.51, Bounding box MAE 5.21e-03\n",
            "Epoch 3, Class accuracy 99.51, Bounding box MAE 5.10e-03\n",
            "Epoch 4, Class accuracy 99.51, Bounding box MAE 5.01e-03\n",
            "Epoch 5, Class accuracy 99.51, Bounding box MAE 4.93e-03\n",
            "Epoch 6, Class accuracy 99.52, Bounding box MAE 4.85e-03\n",
            "Epoch 7, Class accuracy 99.52, Bounding box MAE 4.76e-03\n",
            "Epoch 8, Class accuracy 99.53, Bounding box MAE 4.65e-03\n",
            "Epoch 9, Class accuracy 99.54, Bounding box MAE 4.52e-03\n",
            "Epoch 10, Class accuracy 99.55, Bounding box MAE 4.39e-03\n",
            "Epoch 11, Class accuracy 99.57, Bounding box MAE 4.27e-03\n",
            "Epoch 12, Class accuracy 99.59, Bounding box MAE 4.17e-03\n",
            "Epoch 13, Class accuracy 99.60, Bounding box MAE 4.08e-03\n",
            "Epoch 14, Class accuracy 99.61, Bounding box MAE 4.00e-03\n",
            "Epoch 15, Class accuracy 99.62, Bounding box MAE 3.93e-03\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "train_iter, _ = load_data_bananas(batch_size)\n",
        "def box_corner_to_center(boxes):\n",
        "    \"\"\"Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\n",
        "    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
        "    cx = (x1 + x2) / 2\n",
        "    cy = (y1 + y2) / 2\n",
        "    w = x2 - x1\n",
        "    h = y2 - y1\n",
        "    boxes = torch.stack((cx, cy, w, h), axis=-1)\n",
        "    return boxes\n",
        "\n",
        "def box_center_to_corner(boxes):\n",
        "    \"\"\"Convert from (center, width, height) to (upper-left, lower-right).\"\"\"\n",
        "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
        "    x1 = cx - 0.5 * w\n",
        "    y1 = cy - 0.5 * h\n",
        "    x2 = cx + 0.5 * w\n",
        "    y2 = cy + 0.5 * h\n",
        "    boxes = torch.stack((x1, y1, x2, y2), axis=-1)\n",
        "    return boxes\n",
        "\n",
        "def multibox_prior(data, sizes, ratios):\n",
        "    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\n",
        "    in_height, in_width = data.shape[-2:]\n",
        "    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n",
        "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
        "    size_tensor = torch.tensor(sizes, device=device)\n",
        "    ratio_tensor = torch.tensor(ratios, device=device)\n",
        "\n",
        "    offset_h, offset_w = 0.5, 0.5\n",
        "    steps_h = 1.0 / in_height  # Scaled steps in y axis\n",
        "    steps_w = 1.0 / in_width  # Scaled steps in x axis\n",
        "\n",
        "    # Generate all center points for the anchor boxes\n",
        "    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\n",
        "    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\n",
        "    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')\n",
        "    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n",
        "\n",
        "    # Generate `boxes_per_pixel` number of heights and widths that are later\n",
        "    # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\n",
        "    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n",
        "                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n",
        "                   * in_height / in_width  # Handle rectangular inputs\n",
        "    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n",
        "                   sizes[0] / torch.sqrt(ratio_tensor[1:])))\n",
        "    # Divide by 2 to get half height and half width\n",
        "    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\n",
        "                                        in_height * in_width, 1) / 2\n",
        "\n",
        "    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\n",
        "    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\n",
        "    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n",
        "                dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n",
        "    output = out_grid + anchor_manipulations\n",
        "    return output.unsqueeze(0)\n",
        "\n",
        "def box_iou(boxes1, boxes2):\n",
        "    \"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\n",
        "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n",
        "                              (boxes[:, 3] - boxes[:, 1]))\n",
        "    # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n",
        "    # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\n",
        "    areas1 = box_area(boxes1)\n",
        "    areas2 = box_area(boxes2)\n",
        "    # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n",
        "    # boxes1, no. of boxes2, 2)\n",
        "    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n",
        "    # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\n",
        "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
        "    union_areas = areas1[:, None] + areas2 - inter_areas\n",
        "    return inter_areas / union_areas\n",
        "\n",
        "def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n",
        "    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n",
        "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
        "    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n",
        "    # box i and the ground-truth bounding box j\n",
        "    jaccard = box_iou(anchors, ground_truth)\n",
        "    # Initialize the tensor to hold the assigned ground-truth bounding box for\n",
        "    # each anchor\n",
        "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\n",
        "                                  device=device)\n",
        "    # Assign ground-truth bounding boxes according to the threshold\n",
        "    max_ious, indices = torch.max(jaccard, dim=1)\n",
        "    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)\n",
        "    box_j = indices[max_ious >= iou_threshold]\n",
        "    anchors_bbox_map[anc_i] = box_j\n",
        "    col_discard = torch.full((num_anchors,), -1)\n",
        "    row_discard = torch.full((num_gt_boxes,), -1)\n",
        "    for _ in range(num_gt_boxes):\n",
        "        max_idx = torch.argmax(jaccard)  # Find the largest IoU\n",
        "        box_idx = (max_idx % num_gt_boxes).long()\n",
        "        anc_idx = (max_idx / num_gt_boxes).long()\n",
        "        anchors_bbox_map[anc_idx] = box_idx\n",
        "        jaccard[:, box_idx] = col_discard\n",
        "        jaccard[anc_idx, :] = row_discard\n",
        "    return anchors_bbox_map\n",
        "\n",
        "def offset_boxes(anchors, assigned_bb, eps=1e-6):\n",
        "    \"\"\"Transform for anchor box offsets.\"\"\"\n",
        "    c_anc = box_corner_to_center(anchors)\n",
        "    c_assigned_bb = box_corner_to_center(assigned_bb)\n",
        "    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]\n",
        "    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])\n",
        "    offset = torch.cat([offset_xy, offset_wh], axis=1)\n",
        "    return offset\n",
        "\n",
        "def multibox_target(anchors, labels):\n",
        "    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\n",
        "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
        "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
        "    device, num_anchors = anchors.device, anchors.shape[0]\n",
        "    for i in range(batch_size):\n",
        "        label = labels[i, :, :]\n",
        "        anchors_bbox_map = assign_anchor_to_bbox(\n",
        "            label[:, 1:], anchors, device)\n",
        "        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(\n",
        "            1, 4)\n",
        "        # Initialize class labels and assigned bounding box coordinates with\n",
        "        # zeros\n",
        "        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n",
        "                                   device=device)\n",
        "        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n",
        "                                  device=device)\n",
        "        # Label classes of anchor boxes using their assigned ground-truth\n",
        "        # bounding boxes. If an anchor box is not assigned any, we label its\n",
        "        # class as background (the value remains zero)\n",
        "        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n",
        "        bb_idx = anchors_bbox_map[indices_true]\n",
        "        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n",
        "        assigned_bb[indices_true] = label[bb_idx, 1:]\n",
        "        # Offset transformation\n",
        "        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n",
        "        batch_offset.append(offset.reshape(-1))\n",
        "        batch_mask.append(bbox_mask.reshape(-1))\n",
        "        batch_class_labels.append(class_labels)\n",
        "    bbox_offset = torch.stack(batch_offset)\n",
        "    bbox_mask = torch.stack(batch_mask)\n",
        "    class_labels = torch.stack(batch_class_labels)\n",
        "    return (bbox_offset, bbox_mask, class_labels)\n",
        "\n",
        "def offset_inverse(anchors, offset_preds):\n",
        "    \"\"\"Predict bounding boxes based on anchor boxes with predicted offsets.\"\"\"\n",
        "    anc = box_corner_to_center(anchors)\n",
        "    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]\n",
        "    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]\n",
        "    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)\n",
        "    predicted_bbox = box_center_to_corner(pred_bbox)\n",
        "    return predicted_bbox\n",
        "\n",
        "def nms(boxes, scores, iou_threshold):\n",
        "    \"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\n",
        "    B = torch.argsort(scores, dim=-1, descending=True)\n",
        "    keep = []  # Indices of predicted bounding boxes that will be kept\n",
        "    while B.numel() > 0:\n",
        "        i = B[0]\n",
        "        keep.append(i)\n",
        "        if B.numel() == 1: break\n",
        "        iou = box_iou(boxes[i, :].reshape(-1, 4),\n",
        "                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)\n",
        "        inds = torch.nonzero(iou <= iou_threshold).reshape(-1)\n",
        "        B = B[inds + 1]\n",
        "    return torch.tensor(keep, device=boxes.device)\n",
        "\n",
        "def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\n",
        "                       pos_threshold=0.009999999):\n",
        "    \"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\n",
        "    device, batch_size = cls_probs.device, cls_probs.shape[0]\n",
        "    anchors = anchors.squeeze(0)\n",
        "    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\n",
        "    out = []\n",
        "    for i in range(batch_size):\n",
        "        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\n",
        "        conf, class_id = torch.max(cls_prob[1:], 0)\n",
        "        predicted_bb = offset_inverse(anchors, offset_pred)\n",
        "        keep = nms(predicted_bb, conf, nms_threshold)\n",
        "        # Find all non-`keep` indices and set the class to background\n",
        "        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)\n",
        "        combined = torch.cat((keep, all_idx))\n",
        "        uniques, counts = combined.unique(return_counts=True)\n",
        "        non_keep = uniques[counts == 1]\n",
        "        all_id_sorted = torch.cat((keep, non_keep))\n",
        "        class_id[non_keep] = -1\n",
        "        class_id = class_id[all_id_sorted]\n",
        "        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n",
        "\n",
        "        below_min_idx = (conf < pos_threshold)\n",
        "        class_id[below_min_idx] = -1\n",
        "        conf[below_min_idx] = 1 - conf[below_min_idx]\n",
        "        pred_info = torch.cat((class_id.unsqueeze(1),\n",
        "                               conf.unsqueeze(1),\n",
        "                               predicted_bb), dim=1)\n",
        "        out.append(pred_info)\n",
        "    return torch.stack(out)\n",
        "\n",
        "def cls_predictor(num_inputs, num_anchors, num_classes):\n",
        "    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),\n",
        "                     kernel_size=3, padding=1)\n",
        "\n",
        "def bbox_predictor(num_inputs, num_anchors):\n",
        "    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)\n",
        "\n",
        "def forward(x, block):\n",
        "    return block(x)\n",
        "\n",
        "Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\n",
        "Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\n",
        "\n",
        "def flatten_pred(pred):\n",
        "    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)\n",
        "\n",
        "def concat_preds(preds):\n",
        "    return torch.cat([flatten_pred(p) for p in preds], dim=1)\n",
        "\n",
        "def down_sample_blk(in_channels, out_channels):\n",
        "    blk = []\n",
        "    for _ in range(2):\n",
        "        blk.append(nn.Conv2d(in_channels, out_channels,\n",
        "                             kernel_size=3, padding=1))\n",
        "        blk.append(nn.BatchNorm2d(out_channels))\n",
        "        blk.append(nn.ReLU())\n",
        "        in_channels = out_channels\n",
        "    blk.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*blk)\n",
        "\n",
        "def base_net():\n",
        "    blk = []\n",
        "    num_filters = [3, 16, 32, 64]\n",
        "    for i in range(len(num_filters) - 1):\n",
        "        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))\n",
        "    return nn.Sequential(*blk)\n",
        "\n",
        "def get_blk(i):\n",
        "    if i == 0:\n",
        "        blk = base_net()\n",
        "    elif i == 1:\n",
        "        blk = down_sample_blk(64, 128)\n",
        "    elif i == 4:\n",
        "        blk = nn.AdaptiveMaxPool2d((1,1))\n",
        "    else:\n",
        "        blk = down_sample_blk(128, 128)\n",
        "    return blk\n",
        "\n",
        "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
        "    Y = blk(X)\n",
        "    anchors = multibox_prior(Y, sizes=size, ratios=ratio)\n",
        "    cls_preds = cls_predictor(Y)\n",
        "    bbox_preds = bbox_predictor(Y)\n",
        "    return (Y, anchors, cls_preds, bbox_preds)\n",
        "\n",
        "sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],\n",
        "         [0.88, 0.961]]\n",
        "ratios = [[1, 2, 0.5]] * 5\n",
        "num_anchors = len(sizes[0]) + len(ratios[0]) - 1\n",
        "\n",
        "class TinySSD(nn.Module):\n",
        "    def __init__(self, num_classes, **kwargs):\n",
        "        super(TinySSD, self).__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        idx_to_in_channels = [64, 128, 128, 128, 128]\n",
        "        for i in range(5):\n",
        "            # Equivalent to the assignment statement `self.blk_i = get_blk(i)`\n",
        "            setattr(self, f'blk_{i}', get_blk(i))\n",
        "            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],\n",
        "                                                    num_anchors, num_classes))\n",
        "            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],\n",
        "                                                      num_anchors))\n",
        "\n",
        "    def forward(self, X):\n",
        "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
        "        for i in range(5):\n",
        "            # Here `getattr(self, 'blk_%d' % i)` accesses `self.blk_i`\n",
        "            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n",
        "                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\n",
        "                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\n",
        "        anchors = torch.cat(anchors, dim=1)\n",
        "        cls_preds = concat_preds(cls_preds)\n",
        "        cls_preds = cls_preds.reshape(\n",
        "            cls_preds.shape[0], -1, self.num_classes + 1)\n",
        "        bbox_preds = concat_preds(bbox_preds)\n",
        "        return anchors, cls_preds, bbox_preds\n",
        "\n",
        "device, net = try_gpu(), TinySSD(num_classes=1)\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "  def __init__(self, w=0.1, reduction='mean', epsilon = 0.8):\n",
        "      super(CustomLoss, self).__init__()\n",
        "      self.w = w\n",
        "      self.epsilon = epsilon\n",
        "      self.reduction = reduction\n",
        "  def forward(self, inputs, targets):\n",
        "    diff = inputs - targets\n",
        "    abs_diff = torch.abs(diff)\n",
        "    custom_loss = torch.where(abs_diff < self.w,\n",
        "                              self.w*torch.log(abs_diff/self.epsilon + 1),\n",
        "                              (abs_diff-self.w+self.w * torch.log(abs_diff/self.epsilon +1)))\n",
        "    if self.reduction == 'mean':\n",
        "      return custom_loss.mean()\n",
        "    elif self.reduction == 'sum':\n",
        "      return custom_loss.sum()\n",
        "    else:\n",
        "      return custom_loss\n",
        "epsilon = 0.8\n",
        "cls_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "bbox_loss = CustomLoss(reduction='none')\n",
        "\n",
        "def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n",
        "    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n",
        "    cls = cls_loss(cls_preds.reshape(-1, num_classes),\n",
        "                   cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)\n",
        "    bbox = bbox_loss(bbox_preds * bbox_masks,\n",
        "                     bbox_labels * bbox_masks).mean(dim=1)\n",
        "    return cls + bbox\n",
        "\n",
        "def cls_eval(cls_preds, cls_labels):\n",
        "    # Because the class prediction results are on the final dimension,\n",
        "    # `argmax` needs to specify this dimension\n",
        "    return float((cls_preds.argmax(dim=-1).type(\n",
        "        cls_labels.dtype) == cls_labels).sum())\n",
        "\n",
        "def bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n",
        "    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())\n",
        "\n",
        "def train(net, train_iter, num_epochs, device):\n",
        "  cls_acc_all = []\n",
        "  bbox_mae_all = []\n",
        "  net = net.to(device)\n",
        "  for epoch in range(num_epochs):\n",
        "    # Sum of training accuracy, no. of examples in sum of training accuracy,\n",
        "    # Sum of absolute error, no. of examples in sum of absolute error\n",
        "    total_cls = 0\n",
        "    total_cls_labels = 0\n",
        "    total_bbox = 0\n",
        "    total_bbox_labels = 0\n",
        "    net.train()\n",
        "    for features, target in train_iter:\n",
        "        trainer.zero_grad()\n",
        "        X, Y = features.to(device), target.to(device)\n",
        "        # Generate multiscale anchor boxes and predict their classes and\n",
        "        # offsets\n",
        "        anchors, cls_preds, bbox_preds = net(X)\n",
        "        # Label the classes and offsets of these anchor boxes\n",
        "        bbox_labels, bbox_masks, cls_labels = multibox_target(anchors, Y)\n",
        "        # Calculate the loss function using the predicted and labeled values\n",
        "        # of the classes and offsets\n",
        "        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n",
        "                      bbox_masks)\n",
        "        l.mean().backward()\n",
        "        trainer.step()\n",
        "        total_cls += cls_eval(cls_preds, cls_labels)\n",
        "        total_cls_labels += cls_labels.numel()\n",
        "        total_bbox += bbox_eval(bbox_preds, bbox_labels, bbox_masks)\n",
        "        total_bbox_labels += bbox_labels.numel()\n",
        "    cls_acc, bbox_mae = total_cls / total_cls_labels * 100, total_bbox / total_bbox_labels\n",
        "    cls_acc_all.append(cls_acc)\n",
        "    bbox_mae_all.append(bbox_mae)\n",
        "    print(f'Epoch {epoch + 1}, Class accuracy {cls_acc:.2f}, Bounding box MAE {bbox_mae:.2e}')\n",
        "\n",
        "  return cls_acc_all, bbox_mae_all\n",
        "\n",
        "num_epochs = 15\n",
        "cls_acc_all, bbox_mae_all = train(net, train_iter, num_epochs, device) #2 min"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2**"
      ],
      "metadata": {
        "id": "JDk51RG5cyZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBwRy6M7zTFI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download(url, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file, return the local filename.\"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "        return fname\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "def download_extract(url, folder=None):\n",
        "    \"\"\"Download and extract a zip file.\"\"\"\n",
        "    fname = download(url)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip/tar files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "voc_dir = download_extract('http://d2l-data.s3-accelerate.amazonaws.com/VOCtrainval_11-May-2012.tar', 'VOCdevkit/VOC2012') #1 min\n",
        "\n",
        "def read_voc_images(voc_dir, is_train=True):\n",
        "    \"\"\"Read all VOC feature and label images.\"\"\"\n",
        "    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n",
        "                             'train.txt' if is_train else 'val.txt')\n",
        "    mode = torchvision.io.image.ImageReadMode.RGB\n",
        "    with open(txt_fname, 'r') as f:\n",
        "        images = f.read().split()\n",
        "    features, labels = [], []\n",
        "    for i, fname in enumerate(images):\n",
        "        features.append(torchvision.io.read_image(os.path.join(\n",
        "            voc_dir, 'JPEGImages', f'{fname}.jpg')))\n",
        "        labels.append(torchvision.io.read_image(os.path.join(\n",
        "            voc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = read_voc_images(voc_dir, True)\n",
        "\n",
        "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
        "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
        "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
        "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
        "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
        "                [0, 64, 128]]\n",
        "\n",
        "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
        "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
        "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\n",
        "\n",
        "def voc_colormap2label():\n",
        "    \"\"\"Build the mapping from RGB to class indices for VOC labels.\"\"\"\n",
        "    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n",
        "    for i, colormap in enumerate(VOC_COLORMAP):\n",
        "        colormap2label[\n",
        "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
        "    return colormap2label\n",
        "\n",
        "def voc_label_indices(colormap, colormap2label):\n",
        "    \"\"\"Map any RGB values in VOC labels to their class indices.\"\"\"\n",
        "    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
        "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
        "           + colormap[:, :, 2])\n",
        "    return colormap2label[idx]\n",
        "\n",
        "def voc_rand_crop(feature, label, height, width):\n",
        "    \"\"\"Randomly crop both feature and label images.\"\"\"\n",
        "    rect = torchvision.transforms.RandomCrop.get_params(\n",
        "        feature, (height, width))\n",
        "    feature = torchvision.transforms.functional.crop(feature, *rect)\n",
        "    label = torchvision.transforms.functional.crop(label, *rect)\n",
        "    return feature, label\n",
        "\n",
        "class VOCSegDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A customized dataset to load the VOC dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, is_train, crop_size, voc_dir, data_len):\n",
        "        self.transform = torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        self.crop_size = crop_size\n",
        "        self.data_len = data_len\n",
        "        features, labels = read_voc_images(voc_dir, is_train=is_train)\n",
        "        self.features = [self.normalize_image(feature)\n",
        "                         for feature in self.filter(features)]\n",
        "        self.labels = self.filter(labels)\n",
        "        self.colormap2label = voc_colormap2label()\n",
        "\n",
        "    def normalize_image(self, img):\n",
        "        return self.transform(img.float() / 255)\n",
        "\n",
        "    def filter(self, imgs):\n",
        "        return [img for img in imgs if (\n",
        "            img.shape[1] >= self.crop_size[0] and\n",
        "            img.shape[2] >= self.crop_size[1])]\n",
        "    def __getitem__(self, idx):\n",
        "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n",
        "                                       *self.crop_size)\n",
        "        return (feature, voc_label_indices(label, self.colormap2label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.features), self.data_len)\n",
        "\n",
        "def load_data_voc(batch_size, crop_size, data_len):\n",
        "    \"\"\"Load the VOC semantic segmentation dataset.\"\"\"\n",
        "    voc_dir = download_extract('http://d2l-data.s3-accelerate.amazonaws.com/VOCtrainval_11-May-2012.tar', os.path.join(\n",
        "        'VOCdevkit', 'VOC2012'))\n",
        "    num_workers = 2\n",
        "    train_iter = torch.utils.data.DataLoader(\n",
        "        VOCSegDataset(True, crop_size, voc_dir, data_len), batch_size,\n",
        "        shuffle=True, drop_last=True, num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(\n",
        "        VOCSegDataset(False, crop_size, voc_dir, data_len), batch_size,\n",
        "        drop_last=True, num_workers=num_workers)\n",
        "    return train_iter, test_iter"
      ],
      "metadata": {
        "id": "d_Z31Fs0zfMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3402c18-063e-4297-fffb-b962cf1dac20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ../data/VOCtrainval_11-May-2012.tar from http://d2l-data.s3-accelerate.amazonaws.com/VOCtrainval_11-May-2012.tar...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crop_size = (160, 140)\n",
        "batch_size = 60\n",
        "data_len = 700\n",
        "train_iter, test_iter = load_data_voc(batch_size, crop_size, data_len) #1 min\n",
        "\n",
        "class ContextExtractor(nn.Module):\n",
        "    def __init__(self, input_channels, branch_channels):\n",
        "        super().__init__()\n",
        "        self.avgpool1 = nn.AvgPool2d(kernel_size=5, stride=2, padding=2)\n",
        "        self.avgpool2 = nn.AvgPool2d(kernel_size=9, stride=(2,3), padding=4)\n",
        "        self.avgpool3 = nn.AvgPool2d(kernel_size=17, stride=(2,3), padding=8)\n",
        "        self.bn = nn.BatchNorm2d(input_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv = nn.Conv2d(input_channels, branch_channels, kernel_size = 1)\n",
        "    def forward(self, x):\n",
        "      branch1 = self.avgpool1(x)\n",
        "      branch1 = self.bn(branch1)\n",
        "      branch1 = self.relu(branch1)\n",
        "      branch1 = self.conv(branch1)\n",
        "      branch2 = self.avgpool2(x)\n",
        "      branch2 = self.bn(branch2)\n",
        "      branch2 = self.relu(branch2)\n",
        "      branch2 = self.conv(branch2)\n",
        "      branch3 = self.avgpool3(x)\n",
        "      branch3 = self.bn(branch3)\n",
        "      branch3 = self.relu(branch3)\n",
        "      branch3 = self.conv(branch3)\n",
        "      add = branch1 + branch2\n",
        "      concatenate = torch.cat((add, branch3), dim=1)\n",
        "      return concatenate\n",
        "\n",
        "\n",
        "class ModifiedGoogLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=21):\n",
        "        super().__init__()\n",
        "        self.pretrained_net = torchvision.models.googlenet(weights=torchvision.models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "        self.net = nn.Sequential(*list(self.pretrained_net.children())[:-3])\n",
        "        #print(self.net) # 15 layers\n",
        "        self.context1 = ContextExtractor(input_channels=832, branch_channels=64)\n",
        "        self.context2 = ContextExtractor(input_channels=1024, branch_channels=64)\n",
        "        self.conv = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "      for i, stage in enumerate(self.net):\n",
        "        stage_output = stage(x)\n",
        "        x = stage_output\n",
        "        if i == 14:\n",
        "          penultimate_stage_output = stage_output\n",
        "      penultimate_stage_output = self.context1(penultimate_stage_output)\n",
        "      stage_output = self.context2(stage_output)\n",
        "      concatenate = torch.cat((stage_output, penultimate_stage_output), dim=1)\n",
        "      return F.interpolate(concatenate, mode='bilinear', size=(160, 140))\n",
        "      return x\n",
        "\n",
        "x = torch.rand(1,3,160,140)\n",
        "model = ModifiedGoogLeNet(num_classes=21)\n",
        "# pretrained_net = torchvision.models.googlenet(weights=torchvision.models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "# net = nn.Sequential(*list(pretrained_net.children())[:-3])\n",
        "# for layer in net:\n",
        "#     x = layer(x)\n",
        "#     print(layer.__class__.__name__, 'output shape:\\t', x.shape)\n",
        "# output of penultimate layer (14) : 832\n",
        "# output of last layer (15) : 1024\n",
        "\n",
        "\n",
        "def accuracy(y_hat, y):\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = torch.argmax(y_hat, axis=1)\n",
        "    cmp = y_hat.type(y.dtype) == y\n",
        "    return float(cmp.type(y.dtype).sum())\n",
        "\n",
        "def evaluate_accuracy(net, data_iter, device=None):\n",
        "    if isinstance(net, nn.Module):\n",
        "        net.eval()  # Set the model to evaluation mode\n",
        "        if not device:\n",
        "            device = next(iter(net.parameters())).device\n",
        "    # No. of correct predictions, no. of predictions\n",
        "    total_hits = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            total_hits += accuracy(net(X), y)\n",
        "            total_samples += y.numel()\n",
        "    return total_hits / total_samples * 100\n",
        "\n",
        "def train_batch(net, X, y, loss, trainer, device):\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    net.train()\n",
        "    trainer.zero_grad()\n",
        "    pred = net(X)\n",
        "    l = loss(pred, y)\n",
        "    l.sum().backward()\n",
        "    trainer.step()\n",
        "    train_loss_sum = l.sum()\n",
        "    train_acc_sum = accuracy(pred, y)\n",
        "    return train_loss_sum, train_acc_sum\n",
        "\n",
        "def train(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
        "               device=try_gpu()):\n",
        "    num_batches = len(train_iter)\n",
        "    train_loss_all = []\n",
        "    train_acc_all = []\n",
        "    test_acc_all = []\n",
        "    net = net.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Sum of training loss, sum of training accuracy, no. of examples,\n",
        "        # no. of predictions\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        total_samples = 0\n",
        "        total_predictions = 0\n",
        "        for i, (features, labels) in enumerate(train_iter):\n",
        "            l, acc = train_batch(\n",
        "                net, features, labels, loss, trainer, device)\n",
        "            total_loss += l\n",
        "            total_acc += acc\n",
        "            total_samples += labels.shape[0]\n",
        "            total_predictions += labels.numel()\n",
        "        train_loss_all.append(total_loss / total_samples)\n",
        "        train_acc_all.append(total_acc / total_predictions * 100)\n",
        "        test_acc = evaluate_accuracy(net, test_iter)\n",
        "        test_acc_all.append(test_acc)\n",
        "        print(f'loss {total_loss / total_samples:.3f}, train acc '\n",
        "            f'{total_acc / total_predictions * 100:.3f}, test acc {test_acc:.3f}')\n",
        "    return train_acc_all, test_acc_all\n",
        "\n",
        "def loss(inputs, targets):\n",
        "    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n",
        "\n",
        "num_epochs, lr, wd, device = 1, 0.6, 0.005, try_gpu()\n",
        "trainer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "train_acc_all, test_acc_all = train(model, train_iter, test_iter, loss, trainer, num_epochs, device) #6 min\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iyuVEh2zmo_",
        "outputId": "abd47b4e-af86-4c8d-a79e-13f1ec9cba79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.0431,  0.0431,  0.0431,  ...,  0.2574,  0.2574,  0.2574],\n",
              "          [ 0.0431,  0.0431,  0.0431,  ...,  0.2574,  0.2574,  0.2574],\n",
              "          [ 0.0431,  0.0431,  0.0431,  ...,  0.2574,  0.2574,  0.2574],\n",
              "          ...,\n",
              "          [ 0.0306,  0.0306,  0.0306,  ...,  0.1000,  0.1000,  0.1000],\n",
              "          [ 0.0306,  0.0306,  0.0306,  ...,  0.1000,  0.1000,  0.1000],\n",
              "          [ 0.0306,  0.0306,  0.0306,  ...,  0.1000,  0.1000,  0.1000]],\n",
              "\n",
              "         [[ 0.1066,  0.1066,  0.1066,  ..., -0.0249, -0.0249, -0.0249],\n",
              "          [ 0.1066,  0.1066,  0.1066,  ..., -0.0249, -0.0249, -0.0249],\n",
              "          [ 0.1066,  0.1066,  0.1066,  ..., -0.0249, -0.0249, -0.0249],\n",
              "          ...,\n",
              "          [-0.0368, -0.0368, -0.0368,  ...,  0.1247,  0.1247,  0.1247],\n",
              "          [-0.0368, -0.0368, -0.0368,  ...,  0.1247,  0.1247,  0.1247],\n",
              "          [-0.0368, -0.0368, -0.0368,  ...,  0.1247,  0.1247,  0.1247]],\n",
              "\n",
              "         [[ 0.3049,  0.3049,  0.3049,  ...,  1.1292,  1.1292,  1.1292],\n",
              "          [ 0.3049,  0.3049,  0.3049,  ...,  1.1292,  1.1292,  1.1292],\n",
              "          [ 0.3049,  0.3049,  0.3049,  ...,  1.1292,  1.1292,  1.1292],\n",
              "          ...,\n",
              "          [ 0.0381,  0.0381,  0.0381,  ..., -0.1170, -0.1170, -0.1170],\n",
              "          [ 0.0381,  0.0381,  0.0381,  ..., -0.1170, -0.1170, -0.1170],\n",
              "          [ 0.0381,  0.0381,  0.0381,  ..., -0.1170, -0.1170, -0.1170]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 0.0274,  0.0274,  0.0274,  ...,  0.0274,  0.0274,  0.0274],\n",
              "          [ 0.0274,  0.0274,  0.0274,  ...,  0.0274,  0.0274,  0.0274],\n",
              "          [ 0.0274,  0.0274,  0.0274,  ...,  0.0274,  0.0274,  0.0274],\n",
              "          ...,\n",
              "          [ 0.0274,  0.0274,  0.0274,  ...,  0.0274,  0.0274,  0.0274],\n",
              "          [ 0.0274,  0.0274,  0.0274,  ...,  0.0274,  0.0274,  0.0274],\n",
              "          [ 0.0274,  0.0274,  0.0274,  ...,  0.0274,  0.0274,  0.0274]],\n",
              "\n",
              "         [[-0.0110, -0.0110, -0.0110,  ..., -0.0110, -0.0110, -0.0110],\n",
              "          [-0.0110, -0.0110, -0.0110,  ..., -0.0110, -0.0110, -0.0110],\n",
              "          [-0.0110, -0.0110, -0.0110,  ..., -0.0110, -0.0110, -0.0110],\n",
              "          ...,\n",
              "          [-0.0110, -0.0110, -0.0110,  ..., -0.0110, -0.0110, -0.0110],\n",
              "          [-0.0110, -0.0110, -0.0110,  ..., -0.0110, -0.0110, -0.0110],\n",
              "          [-0.0110, -0.0110, -0.0110,  ..., -0.0110, -0.0110, -0.0110]],\n",
              "\n",
              "         [[ 0.0235,  0.0235,  0.0235,  ...,  0.0235,  0.0235,  0.0235],\n",
              "          [ 0.0235,  0.0235,  0.0235,  ...,  0.0235,  0.0235,  0.0235],\n",
              "          [ 0.0235,  0.0235,  0.0235,  ...,  0.0235,  0.0235,  0.0235],\n",
              "          ...,\n",
              "          [ 0.0235,  0.0235,  0.0235,  ...,  0.0235,  0.0235,  0.0235],\n",
              "          [ 0.0235,  0.0235,  0.0235,  ...,  0.0235,  0.0235,  0.0235],\n",
              "          [ 0.0235,  0.0235,  0.0235,  ...,  0.0235,  0.0235,  0.0235]]]],\n",
              "       grad_fn=<UpsampleBilinear2DBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3**"
      ],
      "metadata": {
        "id": "-1Vq5ln0c1kV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJXMMTSrZoIW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from PIL import Image\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download(url, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file, return the local filename.\"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "        return fname\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "def download_extract(url, folder=None):\n",
        "    \"\"\"Download and extract a zip file.\"\"\"\n",
        "    fname = download(url, cache_dir=\".\")\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    fp = zipfile.ZipFile(fname, 'r')\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip/tar files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "download_extract('https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip', 'Flickr8k_Dataset')\n",
        "download_extract('https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip', 'Flickr8k_text')\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<BOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def tokenizer_eng(self, text):\n",
        "        return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "def load_captions(path):\n",
        "    captions_list = []\n",
        "    image_captions = {}\n",
        "    with open(path, \"r\") as file:\n",
        "        for line in file.readlines():\n",
        "            words = line.strip(\"\\n\").split()\n",
        "            caption = ' '.join(words[1:])\n",
        "            captions_list.append(caption)\n",
        "\n",
        "    return captions_list\n",
        "\n",
        "vocab = Vocabulary(freq_threshold=5)\n",
        "vocab.build_vocabulary(load_captions(\"Flickr8k.token.txt\"))\n",
        "\n",
        "def split_data(all_captions_path, img_list_path, name):\n",
        "    img_names = []\n",
        "    with open(img_list_path, \"r\") as file:\n",
        "        for line in file.readlines():\n",
        "            img_name = line.strip(\"\\n\").split(\".\")[0]\n",
        "            img_names.append(img_name)\n",
        "\n",
        "    lines = []\n",
        "    with open(all_captions_path, \"r\") as file:\n",
        "        for line in file.readlines():\n",
        "            words = line.replace(\";\",\",\").strip(\"\\n\").split()\n",
        "            img_name = words[0].split(\".\")[0]\n",
        "\n",
        "            if img_name in img_names:\n",
        "                new_line = img_name + \".jpg;\" + \" \".join(words[1:])\n",
        "                lines.append(new_line)\n",
        "\n",
        "    with open(name, \"w\") as file:\n",
        "        file.writelines(\"image;caption\\n\")\n",
        "        lines = map(lambda x:x + '\\n', lines)\n",
        "        file.writelines(lines)\n",
        "\n",
        "split_data(\"Flickr8k.token.txt\", \"Flickr_8k.trainImages.txt\", \"train.txt\")\n",
        "split_data(\"Flickr8k.token.txt\", \"Flickr_8k.devImages.txt\", \"validation.txt\")\n",
        "split_data(\"Flickr8k.token.txt\", \"Flickr_8k.testImages.txt\", \"test.txt\")\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, vocab, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file, sep=\";\")\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get img, caption columns\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<BOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "class CollateDataset:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets\n",
        "\n",
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    vocab,\n",
        "    transform,\n",
        "    batch_size=32,\n",
        "    num_workers=2,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, vocab, transform=transform)\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=CollateDataset(pad_idx=dataset.vocab.stoi[\"<PAD>\"]),\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n",
        "\n",
        "transform = transforms.Compose(\n",
        "        [transforms.Resize((299, 299)),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
        "         ]\n",
        "    )\n",
        "\n",
        "train_loader, train_dataset = get_loader(\n",
        "    \"Flicker8k_Dataset\", \"train.txt\", vocab, transform=transform\n",
        ")\n",
        "\n",
        "val_loader, val_dataset = get_loader(\n",
        "    \"Flicker8k_Dataset\", \"validation.txt\", vocab, transform=transform\n",
        ")\n",
        "\n",
        "test_loader, test_dataset = get_loader(\n",
        "    \"Flicker8k_Dataset\", \"test.txt\", vocab, transform=transform, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "PJW0ElPcapRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.train_CNN = train_CNN\n",
        "        self.mnasnet0_5 = models.mnasnet0_5(weights=models.MNASNet0_5_Weights.IMAGENET1K_V1)\n",
        "        self.mnasnet0_5.fc = nn.Linear(self.mnasnet0_5.fc.in_features, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.inception(images)\n",
        "        return self.dropout(self.relu(features))\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers)\n",
        "        nn.init.xavier_uniform_(self.gru.weight)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        nn.init.kaiming_uniform(self.linear.weight)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        _, states = self.gru(features.unsqueeze(0))\n",
        "        hiddens, _ = self.gru(embeddings, states)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "class CNNtoRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=40):\n",
        "        result_caption = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(0)\n",
        "                result_caption.append(predicted.item())\n",
        "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "                    break\n",
        "\n",
        "        return [vocabulary.itos[idx] for idx in result_caption]\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "vocab_size = len(vocab)\n",
        "num_layers = 1\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 1\n",
        "device = try_gpu()\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
        "\n",
        "for name, param in model.encoderCNN.inception.named_parameters():\n",
        "        if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "train_loss = [] #5 min\n",
        "dev_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    train_run_loss = []\n",
        "    dev_run_loss = []\n",
        "    index = 0\n",
        "\n",
        "    for idx, (imgs, captions) in enumerate(train_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        captions = captions.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs, captions)\n",
        "        loss = loss_criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_run_loss.append(loss.item())\n",
        "        if (index+1) % 100 == 0:\n",
        "            print(\"[Train {}] Iteration {} | Running Loss: {}\".format(epoch+1, index+1, round(np.mean(train_run_loss), 4)))\n",
        "        index += 1\n",
        "\n",
        "    avg_loss = np.mean(train_run_loss)\n",
        "    train_loss.append(avg_loss)\n",
        "    print(\"[Train] Epoch {} | Average Loss: {}\".format(epoch+1, round(avg_loss, 4)))\n",
        "\n",
        "    index = 0\n",
        "    for idx, (imgs, captions) in enumerate(val_loader):\n",
        "        with torch.no_grad():\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "            outputs = model(imgs, captions)\n",
        "            loss = loss_criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
        "            dev_run_loss.append(loss.item())\n",
        "            index += 1\n",
        "\n",
        "    avg_loss = np.mean(dev_run_loss)\n",
        "    dev_loss.append(avg_loss)\n",
        "    print(\"[Valid] Epoch {} | Average Loss: {}\".format(epoch+1, round(avg_loss, 4)))\n",
        "\n"
      ],
      "metadata": {
        "id": "Cvhq04BvbZLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4**"
      ],
      "metadata": {
        "id": "X9eZxAqTc3oV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOCbeZEqqaca"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "import warnings\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
        "    \"\"\"Plot a list of images.\"\"\"\n",
        "    figsize = (num_cols * scale, num_rows * scale)\n",
        "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "    plt.tight_layout()\n",
        "    axes = axes.flatten()\n",
        "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
        "        if torch.is_tensor(img):\n",
        "            # Tensor Image\n",
        "            ax.imshow(img.numpy())\n",
        "        else:\n",
        "            # PIL Image\n",
        "            ax.imshow(img)\n",
        "        ax.axes.get_xaxis().set_visible(False)\n",
        "        ax.axes.get_yaxis().set_visible(False)\n",
        "        if titles:\n",
        "            ax.set_title(titles[i])\n",
        "    return axes\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "trans = transforms.ToTensor()\n",
        "mnist_train = torchvision.datasets.FashionMNIST(\n",
        "    root=\"../data\", train=True, transform=trans, download=True)\n",
        "mnist_test = torchvision.datasets.FashionMNIST(\n",
        "    root=\"../data\", train=False, transform=trans, download=True)\n",
        "#batch_size = 256\n",
        "#train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size,\n",
        "                                        # shuffle=True, num_workers=2)\n",
        "\n",
        "transformer = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((36, 36)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(0.5, 0.5),\n",
        "    torchvision.transforms.RandomVerticalFlip(),\n",
        "    torchvision.transforms.ColorJitter(\n",
        "    brightness=0, contrast=0, saturation=0, hue=0.1)\n",
        "])\n",
        "mnist_train.transform = transformer"
      ],
      "metadata": {
        "id": "AoFUZQ4PqgeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09505b06-ebf7-4434-904b-4608940f5955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 26.4M/26.4M [00:01<00:00, 13.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 29.5k/29.5k [00:00<00:00, 209kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 4.42M/4.42M [00:01<00:00, 3.48MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5.15k/5.15k [00:00<00:00, 18.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "data_iter = torch.utils.data.DataLoader(\n",
        "    mnist_train, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=2)\n",
        "\n",
        "def update_D(X, Z, net_D, net_G, loss, trainer_D):\n",
        "    \"\"\"Update discriminator.\"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    ones = torch.ones((batch_size,), device=X.device)\n",
        "    zeros = torch.zeros((batch_size,), device=X.device)\n",
        "    trainer_D.zero_grad()\n",
        "    real_Y = net_D(X)\n",
        "    fake_X = net_G(Z)\n",
        "    fake_Y = net_D(fake_X.detach())\n",
        "    real_Y = real_Y.reshape(batch_size, -1).mean(dim=1)  # Average over all dimensions except batch size\n",
        "    fake_Y = fake_Y.reshape(batch_size, -1).mean(dim=1)  # Average over all dimensions except batch size\n",
        "    loss_D = (loss(real_Y, ones) + loss(fake_Y, zeros)) / 2\n",
        "    loss_D.backward()\n",
        "    trainer_D.step()\n",
        "    return loss_D\n",
        "\n",
        "def update_G(Z, net_D, net_G, loss, trainer_G):\n",
        "    \"\"\"Update generator.\"\"\"\n",
        "    batch_size = Z.shape[0]\n",
        "    ones = torch.ones((batch_size,), device=Z.device)\n",
        "    trainer_G.zero_grad()\n",
        "    # We could reuse `fake_X` from `update_D` to save computation\n",
        "    fake_X = net_G(Z)\n",
        "    # Recomputing `fake_Y` is needed since `net_D` is changed\n",
        "    fake_Y = net_D(fake_X)\n",
        "    fake_Y = fake_Y.reshape(batch_size, -1).mean(dim=1)  # Average over all dimensions except batch size\n",
        "    # calculate loss_G using reshaped fake_Y\n",
        "    loss_G = loss(fake_Y, ones)\n",
        "    loss_G.backward()\n",
        "    trainer_G.step()\n",
        "    return loss_G\n",
        "\n",
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()\n",
        "\n",
        "class Animator:\n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
        "                 figsize=(3.5, 2.5)):\n",
        "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [self.axes, ]\n",
        "        # Use a lambda function to capture arguments\n",
        "        self.config_axes = lambda: set_axes(\n",
        "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "class G_block(nn.Module):\n",
        "    def __init__(self, out_channels, in_channels=1, kernel_size=4, strides=2,\n",
        "                 padding=1, **kwargs):\n",
        "        super(G_block, self).__init__(**kwargs)\n",
        "        self.conv2d_trans = nn.ConvTranspose2d(in_channels, out_channels,\n",
        "                                kernel_size, strides, padding, bias=False)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.conv2d_trans(X)\n",
        "        X = self.batch_norm(X)\n",
        "        X = self.activation(X)\n",
        "        return X\n",
        "\n",
        "n_G = 32\n",
        "net_G = nn.Sequential(\n",
        "    G_block(in_channels=90, out_channels=n_G*8,\n",
        "            strides=1, padding=0),\n",
        "    G_block(in_channels=n_G*8, out_channels=n_G*4),\n",
        "    G_block(in_channels=n_G*4, out_channels=n_G*2),\n",
        "    G_block(in_channels=n_G*2, out_channels=n_G),\n",
        "    nn.ConvTranspose2d(in_channels=n_G, out_channels=3,\n",
        "                       kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.Tanh())  # Output: (3, 64, 64)\n",
        "net_G\n",
        "\n",
        "class D_block(nn.Module):\n",
        "    def __init__(self, out_channels, in_channels=1, **kwargs): # Change in_channels to 1\n",
        "        super(D_block, self).__init__(**kwargs)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size = 1, bias=False) # Change in_channels to 1\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2d_1 = nn.Conv2d(out_channels//3, out_channels, kernel_size = 3, stride = 2,  bias=False)\n",
        "        self.conv2d_2 = nn.Conv2d(out_channels//3, out_channels, kernel_size = 3, stride = 2,  bias=False)\n",
        "        self.conv2d_3 = nn.Conv2d(out_channels//3, out_channels, kernel_size = 3, stride = 2, bias=False)\n",
        "\n",
        "        self.conv2d_final = nn.Conv2d(3*out_channels, out_channels, kernel_size = 1, bias=False)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.conv2d(x)\n",
        "        x = self.relu1(x)\n",
        "        branch1, branch2, branch3 = torch.split(x, x.shape[1] // 3, dim=1)\n",
        "\n",
        "        branch1 = self.conv2d_1(branch1)\n",
        "        branch2 = self.conv2d_2(branch2)\n",
        "        branch2 = self.relu2(branch2)\n",
        "        branch3 = self.conv2d_3(branch3)\n",
        "\n",
        "        x = torch.cat([branch1,branch2, branch3], dim=1)\n",
        "        x = self.conv2d_final(x)\n",
        "        x = self.batch_norm(x)\n",
        "        return x\n",
        "\n",
        "n_D = 96\n",
        "net_D = nn.Sequential(\n",
        "    D_block(n_D),\n",
        "    D_block(in_channels=n_D, out_channels=n_D * 2),\n",
        "    D_block(in_channels=n_D * 2, out_channels=n_D * 4),\n",
        "    nn.Conv2d(in_channels=n_D * 4, out_channels=1, kernel_size=2, bias=False)\n",
        ")\n",
        "def train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,\n",
        "          device=try_gpu()):\n",
        "    loss = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "    for w in net_D.parameters():\n",
        "        nn.init.normal_(w, 0, 0.02)\n",
        "    for w in net_G.parameters():\n",
        "        nn.init.normal_(w, 0, 0.02)\n",
        "    net_D, net_G = net_D.to(device), net_G.to(device)\n",
        "    trainer_hp = {'lr': lr, 'betas': [0.5,0.999]}\n",
        "    trainer_D = torch.optim.Adam(net_D.parameters(), **trainer_hp)\n",
        "    trainer_G = torch.optim.Adam(net_G.parameters(), **trainer_hp)\n",
        "    animator = Animator(xlabel='epoch', ylabel='loss',\n",
        "                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n",
        "                            legend=['discriminator', 'generator'])\n",
        "    animator.fig.subplots_adjust(hspace=0.3)\n",
        "    D_loss = 0\n",
        "    G_loss = 0\n",
        "    total_examples = 0\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # Train one epoch\n",
        "        for X, _ in data_iter:\n",
        "            batch_size = X.shape[0]\n",
        "            Z = torch.normal(0, 1, size=(batch_size, latent_dim, 1, 1))\n",
        "            X, Z = X.to(device), Z.to(device)\n",
        "            D_loss += update_D(X, Z, net_D, net_G, loss, trainer_D)\n",
        "            G_loss += update_G(Z, net_D, net_G, loss, trainer_G)\n",
        "            total_examples += batch_size\n",
        "        # Show generated examples\n",
        "        Z = torch.normal(0, 1, size=(21, latent_dim, 1, 1), device=device)\n",
        "        # Normalize the synthetic data to N(0, 1)\n",
        "        fake_x = net_G(Z).permute(0, 2, 3, 1) / 2 + 0.5\n",
        "        imgs = torch.cat(\n",
        "            [torch.cat([\n",
        "                fake_x[i * 7 + j].cpu().detach() for j in range(7)], dim=1)\n",
        "             for i in range(len(fake_x)//7)], dim=0)\n",
        "        animator.axes[1].cla()\n",
        "        animator.axes[1].imshow(imgs)\n",
        "        # Show the losses\n",
        "        loss_D, loss_G = D_loss / total_examples, G_loss / total_examples\n",
        "        animator.add(epoch, (loss_D.cpu().detach().numpy(), loss_G.cpu().detach().numpy()))\n",
        "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, ')\n",
        "\n",
        "latent_dim, lr, num_epochs = 90, 0.05, 1\n",
        "train(net_D, net_G, data_iter, num_epochs, lr, latent_dim)"
      ],
      "metadata": {
        "id": "FXNpEwq0q8KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5**"
      ],
      "metadata": {
        "id": "tFzl3t6Kc5pr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmpJCXCk9W7B",
        "outputId": "37944366-7f88-4002-db78-b42d321f82e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-14.0.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision\n",
        "from torchvision.models.video import r3d_18\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "torch.manual_seed(42);\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HMDB51\n",
        "!pip install av\n",
        "!wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar #1 min\n",
        "!wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar\n",
        "!mkdir -p video_data test_train_splits\n",
        "!unrar e test_train_splits.rar test_train_splits\n",
        "!rm test_train_splits.rar\n",
        "!unrar e hmdb51_org.rar\n",
        "!rm hmdb51_org.rar\n",
        "!mv *.rar video_data\n",
        "for files in os.listdir('video_data'):\n",
        "    foldername = files.split('.')[0]\n",
        "    os.system(\"mkdir -p video_data/\" + foldername)\n",
        "    os.system(\"unrar e video_data/\"+ files + \" video_data/\" + foldername)\n",
        "\n",
        "!rm video_data/*.rar\n",
        "val_split = 0.05 #3 min\n",
        "num_frames = 16\n",
        "clip_steps = 50\n",
        "num_workers = 8\n",
        "bs = 4\n",
        "\n",
        "train_tfms = torchvision.transforms.Compose([\n",
        "                                 transforms.Lambda(lambda x: x.permute(3, 0, 1, 2).to(torch.float32) / 255.),\n",
        "                                 transforms.Resize((128, 171)),\n",
        "                                 transforms.RandomHorizontalFlip(),\n",
        "                                 transforms.RandomCrop((112, 112))\n",
        "                               ])\n",
        "test_tfms = torchvision.transforms.Compose([\n",
        "                                             transforms.Lambda(lambda x: x.permute(3, 0, 1, 2).to(torch.float32) / 255.),\n",
        "                                             transforms.Resize((128, 171)),\n",
        "                                             transforms.CenterCrop((112, 112))\n",
        "                                             ])\n",
        "\n",
        "hmdb51_train = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames,\n",
        "                                                step_between_clips=clip_steps, fold=1, train=True,\n",
        "                                                transform=train_tfms, num_workers=num_workers)\n",
        "\n",
        "hmdb51_test = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames,\n",
        "                                                step_between_clips=clip_steps, fold=1, train=False,\n",
        "                                                transform=test_tfms, num_workers=num_workers)\n",
        "\n",
        "\n",
        "total_train_samples = len(hmdb51_train)\n",
        "total_val_samples = round(val_split * total_train_samples)\n",
        "\n",
        "hmdb51_train_v1, hmdb51_val_v1 = random_split(hmdb51_train, [total_train_samples - total_val_samples, total_val_samples])\n",
        "\n",
        "train_loader = DataLoader(hmdb51_train_v1, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "val_loader   = DataLoader(hmdb51_val_v1, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "test_loader  = DataLoader(hmdb51_test, batch_size=bs, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')"
      ],
      "metadata": {
        "id": "VUy1_Qd49kKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Inc(nn.Module):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(Inc, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(64, 64, **kwargs)\n",
        "        self.conv2 = nn.Conv3d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv3d(64, 64, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv1(x)\n",
        "        x3 = self.conv1(x)\n",
        "        x2_1 = self.conv2(x2)\n",
        "        x3_1 = self.conv2(x3)\n",
        "        x4 = torch.concat([x1, x2_1, x3_1], dim=1)\n",
        "        x4 = self.conv3(x4)\n",
        "        return x4\n",
        "\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv3d(64, 64, kernel_size=1, stride = 1, padding=1)\n",
        "        self.conv3 = nn.Conv3d(64, 64, kernel_size=3, padding=1, stride=1)\n",
        "        self.inc = Inc(64)\n",
        "        self.avgpool = nn.AvgPool3d(kernel_size=(2,7,7))\n",
        "        self.conv4 = nn.Conv3d(64, 64, kernel_size=1, stride=1, padding=1)\n",
        "        self.adaptive_avgpool = nn.AdaptiveAvgPool3d((1,1,1))\n",
        "        self.bn = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(64, 101)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        for _ in range(9):\n",
        "          x = self.inc(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.adaptive_avgpool(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = InceptionBlock()\n",
        "def train(model, loader, optimizer, epoch, device=try_gpu()):\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    total_loss, total_correct, num_labels = 0, 0, 0\n",
        "    for batch_id, data in enumerate(loader):\n",
        "        data, target = data[0], data[-1]\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_labels += data.size(0)\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
        "        num_corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        total_correct += num_corrects\n",
        "\n",
        "        if (batch_id + 1) % 100 == 0:\n",
        "           print('Train Epoch: {} Batch [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "                epoch, (batch_id + 1) * len(data), len(loader.dataset),\n",
        "                100. * (batch_id + 1) / len(loader), total_loss / num_labels, total_correct, num_labels, 100. * total_correct / num_labels))\n",
        "\n",
        "    print('Train Epoch: {} Average Loss: {:.6f} Average Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "         epoch, total_loss / num_labels, total_correct, num_labels, 100. * total_correct / num_labels))\n",
        "\n",
        "def test(model, loader, text='Validation', device=try_gpu()):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    total_loss, total_correct, num_labels = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "         for batch_id, data in enumerate(loader):\n",
        "             data, target = data[0], data[-1]\n",
        "\n",
        "             data = data.to(device)\n",
        "             target = target.to(device)\n",
        "\n",
        "             output = model(data)\n",
        "             loss = F.cross_entropy(output, target)\n",
        "\n",
        "             total_loss += loss.item()\n",
        "             num_labels += data.size(0)\n",
        "\n",
        "             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
        "             num_corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "             total_correct += num_corrects\n",
        "\n",
        "    print(text + ' Average Loss: {:.6f} Average Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "         total_loss / num_labels, total_correct, num_labels, 100. * total_correct / num_labels))\n",
        "\n",
        "lr = 1e-2\n",
        "gamma = 0.7\n",
        "total_epochs = 1\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "for epoch in range(1, total_epochs + 1):\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test(model, val_loader, text=\"Validation\")\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "bPTLtCvO93Kv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}